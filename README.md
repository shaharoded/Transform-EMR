# Event Prediction in EMRs

This repository implements a two-phase deep learning pipeline for modeling longitudinal Electronic Medical Records (EMRs). The architecture combines temporal embeddings, patient context, and Transformer-based sequence modeling to predict or impute patient events over time.

This model is a part of my thesis and will be used on actual EMR data, stored in a closed environment. For that, it is organized as a package that can be installed:

```bash
event-prediction-in-diabetes-care/
â”‚
â”œâ”€â”€ transform_emr/           # Core Python package
â”‚   â”œâ”€â”€ config/              # Configuration modules
â”‚   â”‚   â”œâ”€â”€ dataset_config.py
â”‚   â”‚   â””â”€â”€ model_config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ dataset.py           # Data preprocessing logic
â”‚   â”œâ”€â”€ embedding.py         # Embedding model (EMREmbedding)
â”‚   â”œâ”€â”€ transformer.py       # Transformer architecture
â”‚   â”œâ”€â”€ train.py             # Training logic
â”‚   â”œâ”€â”€ inference.py         # Inference pipeline
â”‚   â””â”€â”€ utils.py             # Utility functions
â”‚
â”œâ”€â”€ data/                      # External data folder (for synthetic or real EMR)
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ test/
â”‚
â”œâ”€â”€ tests/                     # Unit and integration tests
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Installation

Install the project as an editable package from the **root** directory:

```bash
pip install -e .
```

```python
from transform_emr.dataset import EMRDataset
from transform_emr.train import run_two_phase_training
```

---

## ğŸš€ Usage

### 1. Prepare Dataset and Update Config

```python
import pandas as pd
from transform_emr.dataset import EMRDataset
from transform_emr.config import dataset_config, model_config

# Load data
df = pd.read_csv(dataset_config.TRAIN_TEMPORAL_DATA_FILE)
ctx_df = pd.read_csv(dataset_config.TRAIN_CTX_DATA_FILE)

# Initialize dataset and update config dynamically
ds = EMRDataset(df, ctx_df)
model_config.MODEL_CONFIG["vocab_size"] = len(ds.token2id)
model_config.MODEL_CONFIG["ctx_dim"] = ds.context_df.shape[1]
```

### 2. Train Model

```python
from transform_emr.train import run_two_phase_training
run_two_phase_training()
```

Model checkpoints and scaler are saved under `checkpoints/phase1/` and `checkpoints/phase2/`.

---

## ğŸ§ª Running Tests

Run all tests:

```bash
pytest tests/
```

ğŸ“ If you have **not trained the model yet**, skip inference-related tests:

```bash
pytest tests/test_dataset.py tests/test_embedder.py
```

âš ï¸ Inference tests assume the presence of:
- `checkpoints/phase1/best_embedder.pt`
- `checkpoints/phase1/scaler.pkl`
- `checkpoints/phase2/best_transformer.pt`

To fix size mismatch errors:
```bash
rm -r checkpoints/
# Then rerun training
```

---

## ğŸ“¦ Packaging Notes

To package without data/checkpoints:

```powershell
# Clean up any existing temp folder
Remove-Item -Recurse -Force .\transform_emr_temp -ErrorAction SilentlyContinue

# Recreate the temp folder
New-Item -ItemType Directory -Path .\transform_emr_temp | Out-Null

# Copy only what's needed
Copy-Item -Path .\transform_emr\* -Destination .\transform_emr_temp\transform_emr -Recurse
Copy-Item -Path .\setup.py, .\README.md, .\requirements.txt -Destination .\transform_emr_temp

# Zip it
Compress-Archive -Path .\transform_emr_temp\* -DestinationPath .\emr_model.zip -Force

# Clean up
Remove-Item -Recurse -Force .\transform_emr_temp
```

---

## ğŸ“Œ Notes

- This project uses synthetic EMR data (`data/train/` and `data/test/`).
- For best results, ensure consistent preprocessing when saving/loading models.
- `model_config.py` should only be updated **after** dataset initialization to avoid embedding size mismatches.

---

## ğŸ”„ End-to-End Workflow

Raw EMR Tables
â”‚
â–¼
Per-patient Event Tokenization (with normalized timestamps)
â”‚
â–¼
ğŸ§  Phase 1 â€“ Train EMREmbedding (token + time + patient context)
â”‚
â–¼
ğŸ“š Phase 2 â€“ Pre-train a Transformer decoder over learned embeddings, as a next-token-prediction task.
â”‚
â–¼
â†’ Predict next medical events or missing timeline entries

---

## ğŸ“¦ Module Overview

### 1. **`dataset.py`** â€“ Temporal EMR Preprocessing

| Component            | Role                                                                                             |
|---------------------|--------------------------------------------------------------------------------------------------|
| `EMRDataset`        | Converts raw EMR tables into per-patient token sequences with relative time.                     |
| `_expand_tokens()`  | Generates CONCEPT_VALUE_(START/END) or single tokens from events. Tokenizing using (START/END) for time intervals allows to capture the length of an event (TIRP - a state or trend).                         |
| `collate_emr()`     | Pads sequences and returns tensors: `token_ids`, `time_deltas`, and fixed-length context vector. |

ğŸ“Œ **Why it matters:**  
Medical data varies in density and structure across patients. This dynamic preprocessing handles irregularity while preserving medically-relevant sequencing via `START/END` logic and relative timing.

---

### 2. **`embedding.py`** â€“ EMR Representation Learning

| Component           | Role                                                                                              |
|--------------------|---------------------------------------------------------------------------------------------------|
| `Time2Vec`          | Learns periodic + trend encoding from inter-event durations.                                      |
| `EMREmbedding`      | Combines token, time, and patient context embeddings. Adds `[CTX]` token for global patient info. |
| `train_embedder()`  | Trains the embedding model with teacher-forced next-token prediction.                            |

ğŸ§  **Insight:**  
Phase 1 learns a robust, patient-aware representation of their event sequences. It isolates the core structure of patient timelines without being confounded by the autoregressive depth of Transformers.

---

### 3. **`transformer.py`** â€“ Causal Language Model over EMR Timelines

| Component           | Role                                                                                              |
|--------------------|---------------------------------------------------------------------------------------------------|
| `GPT`               | Transformer decoder stack over learned embeddings.                                                |
| `CausalSelfAttention` | Multi-head attention using causal mask to enforce chronology.                                 |
| `configure_optimizers()` | Groups model parameters for AdamW with correct weight decay policy.                         |

âš™ï¸ **Phase 2: Learning Sequence Dependencies**  
Once the EMR structure is captured, the transformer learns to model sequential dependencies in event progression:  
- What tends to follow a certain event?  
- How does timing affect outcomes?  
- How does patient context modulate the trajectory?

---

## âœ… Model Capabilities

- âœ”ï¸ **Handles irregular time-series data** using relative deltas and Time2Vec.
- âœ”ï¸ **Captures both short- and long-range dependencies** with deep transformer blocks.
- âœ”ï¸ **Supports variable-length patient histories** using custom collate and attention masks.
- âœ”ï¸ **Imputes and predicts** events in structured EMR timelines.

---

## ğŸ“š Citation
Inspired by recent advancements in temporal deep learning, sequence modeling in healthcare (BEHRT, RETAIN, Med-BERT), and Time2Vec (Kazemi et al.).