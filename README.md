This document walks you through every public‑facing class, function, and helper that appears in dataset.py, embedding.py, and transformer.py.


*'1. Data layer – dataset.py'*

| Element                                     | What it does                                                                                                                                                                                                                              | Key inputs                                                                                                                                                                  | Key outputs / state                                                                                                                                                                                                                          |   |
| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | - |
| **`EMRDataset` (torch.utils.data.Dataset)** | Converts raw longitudinal EMR tables into per‑patient event sequences, normalised to “days since admission”. Creates a *token vocabulary* on the fly and attaches patient‑level context (age, gender…).                                   | • `df`: EMR dataframe with `PatientID`, `ConceptName`, dates & value • `patient_context_df` • `numeric_concepts` (events that get **START/END** tokens) • `context_columns` | • `tokens_df` – flat table of every event token with a relative timestamp • `token2id` – lookup dict • `patient_groups` – easy slicing by patient id • implements `__len__` (nr patients) & `__getitem__` (returns tensors for one patient)  |   |
| **`_expand_tokens` (private)**              | Splits each original row into one or two “atomic” tokens.<br>• For *numeric* concepts it produces a pair `<CONCEPT>_<value>_START` and `<CONCEPT>_<value>_END`.<br>• Otherwise maps Boolean or categorical values to a single token name. | One row of the EMR dataframe                                                                                                                                                | A row‑list later converted to `pd.DataFrame` for concatenation                                                                                                                                                                               |   |
| **`collate_emr`**                           | Custom `DataLoader` *collate\_fn* that pads the variable‑length patient sequences to the batch max length and **stacks** the fixed context vector untouched.                                                                              | Batch (list of dicts) plus `pad_token_id`                                                                                                                                   | Dict with `token_ids`, `time_deltas`, `context_vector` ready for the network                                                                                                                                                                 |   |


*'2. Representation layer – embedding.py'*

| Element              | Role                                                                                                                                                                                                                                    | Inputs                                                               | Returns                                                            |   |
| -------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- | ------------------------------------------------------------------ | - |
| **`Time2Vec`**       | Encodes a scalar *time‑delta* into a vector with one **linear** channel (trend) and *k‑1* **periodic** channels (learnable sine frequencies).                                                                                           | `t` – tensor `[B,T]` or `[B*T]` of days                              | `[B,T,k]` where `k = out_dim` (default 8)                          |   |
| **`EMREmbedding`**   | Combines **token embeddings**, **time embeddings** (`Time2Vec`), and a learnable **\[CTX]** token enriched by the fixed patient context. Effectively turns a patient timeline into `[CTX] + event₁ + event₂ …` ready for a Transformer. | • `token_ids [B,T]` • `time_deltas [B,T]` • `patient_contexts [B,C]` | `embeddings [B,T+1,D]` where `D = embed_dim`                       |   |
| **`train` (helper)** | End‑to‑end routine that trains **only the embedding + a linear decoder** with teacher‑forcing language‑model loss. Includes early‑stopping.                                                                                             | PyTorch DataLoaders, model dims, hyper‑params                        | Trained `EMREmbedding`, decoder `nn.Linear`, train & val loss logs |   |


*'3. Sequence model – transformer.py'*

| Element                   | Purpose                                                                                                                                                                                   | Shape/Behaviour                           |                                                                    |   |
| ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | ------------------------------------------------------------------ | - |
| **`LayerNorm`**           | Re‑implements PyTorch `LayerNorm` but allows turning the bias off, mirroring the original GPT code‑base.                                                                                  | Standard LN on last‑dim                   |                                                                    |   |
| **`CausalSelfAttention`** | Multi‑head self‑attention with an *in‑module* causal mask (`torch.tril`) so events can’t peek into the future. Performs QKV projection, masking, dropout, and output projection.          | Accepts `[B,T,D]` and returns same        |                                                                    |   |
| **`MLP`**                 | Two‑layer feed‑forward (4 × width) with GELU and dropout—identical to GPT‑2.                                                                                                              | `[B,T,D] → [B,T,D]`                       |                                                                    |   |
| **`Block`**               | Residual wrapper around **LN ➔ Attention ➔ LN ➔ MLP**.                                                                                                                                    | `[B,T,D]` in / out                        |                                                                    |   |
| **`GPT`**                 | Full decoder‑only Transformer: token & position embedding tables, `n_layer` **Block** stack, final LN‑F, and weight‑tied LM head. Provides `forward` (with optional loss) and `generate`. | • `idx [B,T]` tokens • optional `targets` | tuple `(logits, loss)` or `(logits, None)`; `generate` outputs ids |   |

Key implementation details:
*Positional encoding uses a learned embedding table (wpe) rather than sinusoidal.
*Weight tying – token embed weight shares parameters with the LM head for efficiency.
*Optimizer helper ‑ configure_optimizers groups parameters to apply weight‑decay only to matrices, matching modern training recipes.
